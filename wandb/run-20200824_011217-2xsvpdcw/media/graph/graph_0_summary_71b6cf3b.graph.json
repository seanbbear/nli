{"format": "torch", "nodes": [{"name": "albert", "id": 140607167113592, "class_name": "AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0, inplace=False)\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=768, out_features=768, bias=True)\n  (pooler_activation): Tanh()\n)", "parameters": [], "output_shape": [[12, 512, 768], [12, 768]], "num_parameters": []}, {"name": "dropout", "id": 140607071417568, "class_name": "Dropout(p=0.1, inplace=False)", "parameters": [], "output_shape": [[12, 768]], "num_parameters": []}, {"name": "classifier", "id": 140607071417680, "class_name": "Linear(in_features=768, out_features=4, bias=True)", "parameters": [], "output_shape": [[12, 4]], "num_parameters": []}, {"name": "albert", "id": 140607167112920, "class_name": "AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0, inplace=False)\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=768, out_features=768, bias=True)\n  (pooler_activation): Tanh()\n)", "parameters": [], "output_shape": [[12, 512, 768], [12, 768]], "num_parameters": []}, {"name": "dropout", "id": 140607071417512, "class_name": "Dropout(p=0.1, inplace=False)", "parameters": [], "output_shape": [[12, 768]], "num_parameters": []}, {"name": "classifier", "id": 140607071417624, "class_name": "Linear(in_features=768, out_features=4, bias=True)", "parameters": [], "output_shape": [[12, 4]], "num_parameters": []}], "edges": []}